{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7aaecef",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f8fd143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shap in c:\\users\\mereu\\miniconda3\\lib\\site-packages (0.49.1)\n",
      "Requirement already satisfied: lime in c:\\users\\mereu\\miniconda3\\lib\\site-packages (0.2.0.1)\n",
      "Requirement already satisfied: ollama in c:\\users\\mereu\\miniconda3\\lib\\site-packages (0.6.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\mereu\\miniconda3\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from shap) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from shap) (1.16.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from shap) (1.7.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from shap) (2.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from shap) (25.0)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from shap) (0.62.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from shap) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from shap) (4.15.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from lime) (3.9.2)\n",
      "Requirement already satisfied: scikit-image>=0.12 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from lime) (0.25.2)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from ollama) (0.27.2)\n",
      "Requirement already satisfied: pydantic>=2.9 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from ollama) (2.10.5)\n",
      "Requirement already satisfied: anyio in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from httpx>=0.27->ollama) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from httpx>=0.27->ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from httpx>=0.27->ollama) (1.0.6)\n",
      "Requirement already satisfied: idna in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from httpx>=0.27->ollama) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from httpx>=0.27->ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.14.0)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from numba>=0.54->shap) (0.45.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from pydantic>=2.9->ollama) (2.27.2)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (3.2.1)\n",
      "Requirement already satisfied: pillow>=10.1 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (10.4.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (2025.6.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from scikit-learn->shap) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from scikit-learn->shap) (3.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from matplotlib->lime) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from matplotlib->lime) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from matplotlib->lime) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from matplotlib->lime) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from matplotlib->lime) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from matplotlib->lime) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->lime) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install shap lime ollama python-dotenv\n",
    "#!pip install \"numpy==1.26.4\" --force-reinstall\n",
    "#!pip install \"shap<0.50\" lime ollama python-dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4af5b0",
   "metadata": {},
   "source": [
    "## Imports, path setup, pipeline reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae9ba4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# If the notebook is inside the `demo/` folder,\n",
    "# the project root is one level up.\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if ROOT_DIR not in sys.path:\n",
    "    sys.path.append(ROOT_DIR)\n",
    "\n",
    "# Classic ML stack\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Make sure we reload the latest version of the pipeline module\n",
    "import src.pipeline\n",
    "importlib.reload(src.pipeline)\n",
    "\n",
    "from config.settings import FrameworkConfig\n",
    "from src.pipeline import XAINLGPipeline\n",
    "from src.nlg.few_shot_generator import FewShotGenerator\n",
    "from src.nlg.ollama_client import ollama_llm_call\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6321235e",
   "metadata": {},
   "source": [
    "## Dataset, model, pipeline, basic SHAP explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ce06dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils:[x] Layer 1: Generating explanations\n",
      "INFO:src.utils:  method: shap\n",
      "INFO:src.utils:[x] Layer 2: Normalizing\n",
      "INFO:src.utils:  features: 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 455, test size: 114, features: 30\n",
      "Raw prediction (0=malignant, 1=benign): 1\n",
      "Available keys in result: dict_keys(['instance', 'method', 'explanation', 'base_value', 'prediction', 'ranked_features', 'statements'])\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Breast Cancer Wisconsin dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = list(data.feature_names)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Train a simple RandomForest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 3. Create framework config and pipeline\n",
    "config = FrameworkConfig(verbose=True)\n",
    "config.explainer.top_k_features = 5  # how many top features to use for NLG\n",
    "\n",
    "config.nlg.debug_print_prompt = True\n",
    "\n",
    "\n",
    "pipeline = XAINLGPipeline(\n",
    "    model=model,\n",
    "    data=X_train,\n",
    "    feature_names=feature_names,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, test size: {len(X_test)}, features: {len(feature_names)}\")\n",
    "\n",
    "# 4. Select one instance to explain\n",
    "instance = X_test[0]\n",
    "raw_pred = int(model.predict(instance.reshape(1, -1))[0])\n",
    "print(\"Raw prediction (0=malignant, 1=benign):\", raw_pred)\n",
    "\n",
    "# 5. Run pipeline only up to XAI (no NLG yet)\n",
    "result = pipeline.explain_instance(\n",
    "    instance,\n",
    "    method=\"shap\",\n",
    "    generate_text=False,   # we handle NLG manually in the next cells\n",
    ")\n",
    "\n",
    "print(\"Available keys in result:\", result.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33979e7",
   "metadata": {},
   "source": [
    "## Few-Shot NLG with Ollama (expert vs layman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e8b7bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k ranked features: [('worst area', 0.06724261426271144), ('worst concave points', 0.05740842121890832), ('mean concave points', 0.04227830540602773), ('worst radius', 0.03956915260512325), ('worst perimeter', 0.03026387093604719)]\n",
      "\n",
      "================ LLM INPUT PROMPT ================\n",
      "You are an explainable AI assistant.\n",
      "You receive model predictions and feature attributions (for example, SHAP or LIME values).\n",
      "Your task is to generate a faithful natural language explanation based ONLY on the provided factors.\n",
      "\n",
      "You are generating an explanation for a CLINICIAN or DATA SCIENTIST.\n",
      "General rules:\n",
      "- The FINAL EXPLANATION MUST BE WRITTEN ONLY IN ENGLISH.\n",
      "- Do NOT invent exact numeric percentages, thresholds or counts (like '4%', 'three times higher') that are not explicitly present in the input. Use only qualitative terms such as 'larger', 'smaller', 'high', 'low' when describing magnitude, unless the number is given.\n",
      "- Do NOT introduce new features or variables that are not listed in the input.\n",
      "- Stay faithful to the provided factors and their directions (supports / contradicts / neutral).\n",
      "Additional guidelines for expert explanations:\n",
      "- It is acceptable to refer to 'features', 'contributions', and 'model output' using technical language.\n",
      "- Do NOT change the direction provided: 'supports' must be described as supporting the predicted class, and 'contradicts' as opposing or protective with respect to that class.\n",
      "- Do NOT invent mathematical relationships that are not present in the input (for example, new ratios or combined indices).\n",
      "- Explicitly connect the sign and approximate magnitude of contributions with the final conclusion (for example, 'large positive contribution increases the malignancy score').\n",
      "- Highlight which factors are dominant and whether some factors partly counterbalance the decision.\n",
      "- Keep the explanation between 3 and 6 sentences, structured logically from evidence to conclusion.\n",
      "\n",
      "Below are a few EXAMPLES of good explanations (Input and explanation both in English):\n",
      "\n",
      "Example 1:\n",
      "Input:\n",
      "Prediction: malignant tumor\n",
      "Explanation method: SHAP\n",
      "Top factors:\n",
      "- worst_area = 1200.0 (supports)\n",
      "- mean_concave_points = 0.20 (supports)\n",
      "- worst_radius = 18.0 (supports)\n",
      "\n",
      "Explanation (English):\n",
      "The model predicts a malignant tumor because several morphological features have large positive contributions to the malignancy score. A very large worst area and radius are typical of aggressive lesions and strongly support the malignant class, as reflected by their positive SHAP values. In addition, a higher density of concave points further reinforces this pattern. Taken together, these dominant positive contributions outweigh any smaller opposing effects and push the overall prediction clearly towards malignancy.\n",
      "\n",
      "Example 2:\n",
      "Input:\n",
      "Prediction: benign tumor\n",
      "Explanation method: SHAP\n",
      "Top factors:\n",
      "- worst_area = 420.0 (contradicts)\n",
      "- mean_radius = 8.9 (contradicts)\n",
      "- smoothness_mean = 0.08 (supports)\n",
      "\n",
      "Explanation (English):\n",
      "The model predicts a benign tumor because the largest area and average radius of the lesion are within ranges typically seen in non-aggressive nodules and have negative contributions to the malignancy score. These factors contradict a malignant pattern and dominate the overall attribution. Although smoothness_mean has a small positive contribution, its effect is modest compared with the stronger negative contributions. As a result, the combined evidence favours the benign class.\n",
      "\n",
      "--- New instance ---\n",
      "Prediction: benign tumor\n",
      "Explanation method: shap\n",
      "Top factors:\n",
      "- worst area = 0.067 (supports)\n",
      "- worst concave points = 0.057 (supports)\n",
      "- mean concave points = 0.042 (supports)\n",
      "- worst radius = 0.040 (supports)\n",
      "- worst perimeter = 0.030 (supports)\n",
      "\n",
      "\n",
      "Now write the FINAL EXPLANATION in ENGLISH only. Do not invent new features or exact numerical values.\n",
      "\n",
      "================ END OF PROMPT ==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ LLM INPUT PROMPT ================\n",
      "You are an explainable AI assistant.\n",
      "You receive model predictions and feature attributions (for example, SHAP or LIME values).\n",
      "Your task is to generate a faithful natural language explanation based ONLY on the provided factors.\n",
      "\n",
      "You are generating an explanation for a NON-EXPERT patient.\n",
      "General rules:\n",
      "- The FINAL EXPLANATION MUST BE WRITTEN ONLY IN ENGLISH.\n",
      "- Do NOT invent exact numeric percentages, thresholds or counts (like '4%', 'three times higher') that are not explicitly present in the input. Use only qualitative terms such as 'larger', 'smaller', 'high', 'low' when describing magnitude, unless the number is given.\n",
      "- Do NOT introduce new features or variables that are not listed in the input.\n",
      "- Stay faithful to the provided factors and their directions (supports / contradicts / neutral).\n",
      "Additional guidelines for layman explanations:\n",
      "- Use simple, calm English that a person without medical or technical background can understand.\n",
      "- Avoid technical terms like 'SHAP value', 'attribution', 'model score', 'positive class', or 'negative class'.\n",
      "- Do NOT invent mathematical operations or relationships (for example, 'ratio of means') that are not explicitly mentioned in the input.\n",
      "- Talk about how the size, shape, or structure of the lump tends to increase or decrease the risk.\n",
      "- When a factor is marked as 'supports', say that it increases or supports the predicted outcome. When it is 'contradicts', say that it goes against that outcome or lowers the risk.\n",
      "- Do not promise certainty: use expressions like 'suggests', 'indicates', 'points towards', rather than 'guarantees'.\n",
      "- Keep the explanation between 3 and 6 sentences, and keep sentences reasonably short.\n",
      "\n",
      "Below are a few EXAMPLES of good explanations (Input and explanation both in English):\n",
      "\n",
      "Example 1:\n",
      "Input:\n",
      "Prediction: high risk of breast cancer\n",
      "Explanation method: SHAP\n",
      "Top factors:\n",
      "- worst_area = 1200.0 (supports)\n",
      "- mean_concave_points = 0.20 (supports)\n",
      "- worst_radius = 18.0 (supports)\n",
      "\n",
      "Explanation (English):\n",
      "The model suggests a higher risk of breast cancer because the lump is quite large and its shape is more irregular than what we usually see in harmless findings. These characteristics are similar to those observed in many confirmed cancer cases, so they strongly push the model towards the cancer category. In simple terms, size and shape together make this lump look more suspicious.\n",
      "\n",
      "Example 2:\n",
      "Input:\n",
      "Prediction: low risk of breast cancer\n",
      "Explanation method: SHAP\n",
      "Top factors:\n",
      "- worst_area = 420.0 (contradicts)\n",
      "- mean_radius = 8.9 (contradicts)\n",
      "- smoothness_mean = 0.08 (supports)\n",
      "\n",
      "Explanation (English):\n",
      "The model suggests a low risk of breast cancer because the lump is relatively small and does not show the more aggressive patterns seen in many cancer cases. The size and overall shape of the lump point away from cancer, even if one of the texture measurements slightly increases the risk. Overall, the evidence leans towards the lump being harmless.\n",
      "\n",
      "--- New instance ---\n",
      "Prediction: low risk of breast cancer\n",
      "Explanation method: shap\n",
      "Top factors:\n",
      "- worst area = 0.067 (supports)\n",
      "- worst concave points = 0.057 (supports)\n",
      "- mean concave points = 0.042 (supports)\n",
      "- worst radius = 0.040 (supports)\n",
      "- worst perimeter = 0.030 (supports)\n",
      "\n",
      "\n",
      "Now write the FINAL EXPLANATION in ENGLISH only. Do not invent new features or exact numerical values.\n",
      "\n",
      "================ END OF PROMPT ==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technical explanation (for a clinician or researcher):\n",
      "The model predicts a benign tumor because several morphological features have small positive contributions to the malignancy score. The worst area, concave points, mean concave points, radius, and perimeter all exhibit supportive attributes with relatively low magnitude. These combined small positive contributions outweigh any opposing effects and collectively push the overall prediction towards benignity.\n",
      "\n",
      "Short explanation for a patient:\n",
      "The model suggests a low risk of breast cancer because the lump has a more benign shape and size, which are typical characteristics of harmless findings. The texture measurements also indicate that this lump is less likely to be cancerous, as its structure is consistent with what we see in non-cancerous cases. Overall, these factors collectively point towards a lower risk of breast cancer.\n"
     ]
    }
   ],
   "source": [
    "# === Layer 3: NLG with Ollama (Few-Shot) ===\n",
    "\n",
    "# We slightly lower the temperature for more stable outputs.\n",
    "config.nlg.temperature = 0.2\n",
    "nlg = FewShotGenerator(config.nlg, llm_call_fn=ollama_llm_call)\n",
    "\n",
    "# Take the top-k features from the SHAP explanation\n",
    "top_k = result[\"ranked_features\"][: config.explainer.top_k_features]\n",
    "print(\"Top-k ranked features:\", top_k)\n",
    "\n",
    "# Build \"direction\" labels from the contribution sign\n",
    "directions = []\n",
    "for _, v in top_k:\n",
    "    if v > 0:\n",
    "        directions.append(\"supports\")\n",
    "    elif v < 0:\n",
    "        directions.append(\"contradicts\")\n",
    "    else:\n",
    "        directions.append(\"neutral\")\n",
    "\n",
    "# Map raw prediction 0/1 to domain-specific labels (EN)\n",
    "raw_pred = int(result[\"prediction\"])\n",
    "pred_expert_en = \"malignant tumor\" if raw_pred == 0 else \"benign tumor\"\n",
    "pred_layman_en = \"high risk of breast cancer\" if raw_pred == 0 else \"low risk of breast cancer\"\n",
    "\n",
    "# Shared context used by all generators\n",
    "base_context = {\n",
    "    \"features\": [f for f, _ in top_k],\n",
    "    \"values\": [float(v) for _, v in top_k],\n",
    "    \"directions\": directions,\n",
    "    \"method\": result.get(\"method\", \"shap\"),\n",
    "}\n",
    "\n",
    "# 1) Context for an expert-level explanation\n",
    "context_expert = {\n",
    "    **base_context,\n",
    "    \"prediction\": pred_expert_en,\n",
    "    \"audience\": \"expert\",   # FewShotGenerator will switch to technical style\n",
    "}\n",
    "\n",
    "# 2) Context for a layman-friendly explanation\n",
    "context_layman = {\n",
    "    **base_context,\n",
    "    \"prediction\": pred_layman_en,\n",
    "    \"audience\": \"layman\",   # FewShotGenerator will switch to simple language\n",
    "}\n",
    "\n",
    "# --- Print one example prompt for Few-Shot (expert) ---\n",
    "# few_shot_prompt_expert = nlg.build_few_shot_prompt(context_expert, style=\"expert\")\n",
    "# print(\"=== FEW-SHOT PROMPT (EXPERT) ===\")\n",
    "# print(few_shot_prompt_expert)\n",
    "# print(\"=== END FEW-SHOT PROMPT ===\\n\")\n",
    "\n",
    "text_expert = nlg.generate(context_expert)\n",
    "text_layman = nlg.generate(context_layman)\n",
    "\n",
    "print(\"Technical explanation (for a clinician or researcher):\")\n",
    "print(text_expert)\n",
    "print(\"\\nShort explanation for a patient:\")\n",
    "print(text_layman)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ac5777",
   "metadata": {},
   "source": [
    "## Chain-of-Thought NLG (technical + simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c8a4622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ LLM INPUT PROMPT ================\n",
      "You are an explainable AI assistant.\n",
      "You receive model predictions and feature attributions (for example, SHAP or LIME values).\n",
      "Your task is to reason step by step and then provide a clear explanation of the prediction.\n",
      "\n",
      "Rules:\n",
      "- The final explanation must be written ONLY in English.\n",
      "- Do NOT invent exact numeric percentages or counts that are not in the input.\n",
      "- Do NOT introduce new features that are not listed in the input.\n",
      "- Respect the directions 'supports' / 'contradicts' / 'neutral'.\n",
      "- Use the step-by-step reasoning only as an internal tool; the final explanation should be a clean paragraph.\n",
      "\n",
      "--- Input context ---\n",
      "Prediction: benign tumor\n",
      "Explanation method: shap\n",
      "Top factors:\n",
      "- worst area = 0.067 (supports)\n",
      "- worst concave points = 0.057 (supports)\n",
      "- mean concave points = 0.042 (supports)\n",
      "- worst radius = 0.040 (supports)\n",
      "- worst perimeter = 0.030 (supports)\n",
      "\n",
      "\n",
      "--- Reasoning (you can think step by step here) ---\n",
      "Think step by step about:\n",
      "- What is the predicted outcome?\n",
      "- Which factors have the largest absolute contributions?\n",
      "- Which factors support the prediction and which ones oppose it?\n",
      "- How do these factors jointly justify the final prediction?\n",
      "\n",
      "Do NOT show this internal thought process directly in the final answer.\n",
      "After you finish reasoning, write a concise explanation for the user.\n",
      "\n",
      "--- Final answer ---\n",
      "Now provide ONLY the final explanation in English (3–6 sentences):\n",
      "\n",
      "================ END OF PROMPT ==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chain-of-Thought (technical) ===\n",
      "The predicted outcome is that the tumor is benign. The top factors contributing to this prediction are all related to the characteristics of the worst areas and features of the tumor, including its concave points, radius, and perimeter. These factors all support the prediction of a benign tumor, indicating that the tumor's shape and size are consistent with a non-malignant growth. Overall, the combination of these factors suggests that the tumor is likely to be benign.\n",
      "\n",
      "================ LLM INPUT PROMPT ================\n",
      "You are an explainable AI assistant.\n",
      "You receive model predictions and feature attributions (for example, SHAP or LIME values).\n",
      "Your task is to reason step by step and then provide a clear explanation of the prediction.\n",
      "\n",
      "Rules:\n",
      "- The final explanation must be written ONLY in English.\n",
      "- Do NOT invent exact numeric percentages or counts that are not in the input.\n",
      "- Do NOT introduce new features that are not listed in the input.\n",
      "- Respect the directions 'supports' / 'contradicts' / 'neutral'.\n",
      "- Use the step-by-step reasoning only as an internal tool; the final explanation should be a clean paragraph.\n",
      "\n",
      "--- Input context ---\n",
      "Prediction: low risk of breast cancer\n",
      "Explanation method: shap\n",
      "Top factors:\n",
      "- worst area = 0.067 (supports)\n",
      "- worst concave points = 0.057 (supports)\n",
      "- mean concave points = 0.042 (supports)\n",
      "- worst radius = 0.040 (supports)\n",
      "- worst perimeter = 0.030 (supports)\n",
      "\n",
      "\n",
      "--- Reasoning (you can think step by step here) ---\n",
      "Think step by step about:\n",
      "- What is the predicted outcome?\n",
      "- Which factors have the largest absolute contributions?\n",
      "- Which factors support the prediction and which ones oppose it?\n",
      "- How do these factors jointly justify the final prediction?\n",
      "\n",
      "Do NOT show this internal thought process directly in the final answer.\n",
      "After you finish reasoning, write a concise explanation for the user.\n",
      "\n",
      "--- Final answer ---\n",
      "Now provide ONLY the final explanation in English (3–6 sentences):\n",
      "\n",
      "================ END OF PROMPT ==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Chain-of-Thought (simple) ===\n",
      "Based on the analysis, it appears that the patient has a low risk of developing breast cancer. The most significant factors contributing to this prediction are the worst area, worst concave points, mean concave points, worst radius, and worst perimeter, all of which have positive SHAP values indicating support for the prediction. These features suggest that the patient's tumor is less aggressive and has a more benign morphology, leading to a lower risk of cancer development. Overall, the combination of these factors collectively justifies the predicted low risk of breast cancer.\n"
     ]
    }
   ],
   "source": [
    "from src.nlg.cot_generator import ChainOfThoughtGenerator\n",
    "from src.nlg.ollama_client import ollama_llm_call\n",
    "\n",
    "# CoT generator also uses the same NLGConfig and Ollama client\n",
    "cot_gen = ChainOfThoughtGenerator(config.nlg, llm_call_fn=ollama_llm_call)\n",
    "\n",
    "# CoT explanation for the expert audience\n",
    "context_cot_expert = {\n",
    "    **base_context,\n",
    "    \"prediction\": pred_expert_en,\n",
    "}\n",
    "\n",
    "text_cot_expert = cot_gen.generate(context_cot_expert)\n",
    "\n",
    "print(\"=== Chain-of-Thought (technical) ===\")\n",
    "print(text_cot_expert)\n",
    "\n",
    "# CoT explanation for a simplified prediction label (same mechanism)\n",
    "context_cot_layman = {\n",
    "    **base_context,\n",
    "    \"prediction\": pred_layman_en,\n",
    "}\n",
    "\n",
    "# --- Print one example CoT prompt ---\n",
    "# cot_prompt = cot_gen.build_cot_prompt(context_cot_expert)\n",
    "# print(\"=== CHAIN-OF-THOUGHT PROMPT (EXPERT) ===\")\n",
    "# print(cot_prompt)\n",
    "# print(\"=== END CHAIN-OF-THOUGHT PROMPT ===\\n\")\n",
    "\n",
    "text_cot_layman = cot_gen.generate(context_cot_layman)\n",
    "\n",
    "print(\"\\n=== Chain-of-Thought (simple) ===\")\n",
    "print(text_cot_layman)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020cfe4c",
   "metadata": {},
   "source": [
    "## Self-Consistency NLG (aggregated explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3884384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ LLM INPUT PROMPT ================\n",
      "You are an explainable AI assistant (reasoning path 1).\n",
      "You receive model predictions and feature attributions (for example, SHAP or LIME values).\n",
      "Explain this model prediction independently.\n",
      "\n",
      "Rules:\n",
      "- The explanation must be written ONLY in English.\n",
      "- Do NOT invent exact numeric percentages or counts that are not in the input.\n",
      "- Do NOT introduce new features that are not listed in the input.\n",
      "- Respect the 'supports' / 'contradicts' / 'neutral' directions.\n",
      "- Provide a concise explanation in 3–6 sentences.\n",
      "\n",
      "--- Input context ---\n",
      "Prediction: benign tumor\n",
      "Explanation method: shap\n",
      "Top factors:\n",
      "- worst area = 0.067 (supports)\n",
      "- worst concave points = 0.057 (supports)\n",
      "- mean concave points = 0.042 (supports)\n",
      "- worst radius = 0.040 (supports)\n",
      "- worst perimeter = 0.030 (supports)\n",
      "\n",
      "\n",
      "Now write the explanation:\n",
      "\n",
      "================ END OF PROMPT ==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ LLM INPUT PROMPT ================\n",
      "You are an explainable AI assistant (reasoning path 2).\n",
      "You receive model predictions and feature attributions (for example, SHAP or LIME values).\n",
      "Explain this model prediction independently.\n",
      "\n",
      "Rules:\n",
      "- The explanation must be written ONLY in English.\n",
      "- Do NOT invent exact numeric percentages or counts that are not in the input.\n",
      "- Do NOT introduce new features that are not listed in the input.\n",
      "- Respect the 'supports' / 'contradicts' / 'neutral' directions.\n",
      "- Provide a concise explanation in 3–6 sentences.\n",
      "\n",
      "--- Input context ---\n",
      "Prediction: benign tumor\n",
      "Explanation method: shap\n",
      "Top factors:\n",
      "- worst area = 0.067 (supports)\n",
      "- worst concave points = 0.057 (supports)\n",
      "- mean concave points = 0.042 (supports)\n",
      "- worst radius = 0.040 (supports)\n",
      "- worst perimeter = 0.030 (supports)\n",
      "\n",
      "\n",
      "Now write the explanation:\n",
      "\n",
      "================ END OF PROMPT ==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ LLM INPUT PROMPT ================\n",
      "You are an explainable AI assistant (reasoning path 3).\n",
      "You receive model predictions and feature attributions (for example, SHAP or LIME values).\n",
      "Explain this model prediction independently.\n",
      "\n",
      "Rules:\n",
      "- The explanation must be written ONLY in English.\n",
      "- Do NOT invent exact numeric percentages or counts that are not in the input.\n",
      "- Do NOT introduce new features that are not listed in the input.\n",
      "- Respect the 'supports' / 'contradicts' / 'neutral' directions.\n",
      "- Provide a concise explanation in 3–6 sentences.\n",
      "\n",
      "--- Input context ---\n",
      "Prediction: benign tumor\n",
      "Explanation method: shap\n",
      "Top factors:\n",
      "- worst area = 0.067 (supports)\n",
      "- worst concave points = 0.057 (supports)\n",
      "- mean concave points = 0.042 (supports)\n",
      "- worst radius = 0.040 (supports)\n",
      "- worst perimeter = 0.030 (supports)\n",
      "\n",
      "\n",
      "Now write the explanation:\n",
      "\n",
      "================ END OF PROMPT ==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ LLM INPUT PROMPT ================\n",
      "You are an explainable AI assistant.\n",
      "You are given several independent explanations generated for the same model prediction.\n",
      "Your task is to aggregate them into a single, coherent explanation.\n",
      "\n",
      "Rules:\n",
      "- Write the final explanation ONLY in English.\n",
      "- Do NOT invent exact numeric percentages or counts that are not in the input.\n",
      "- Do NOT introduce new features that are not listed in the context or in the explanations.\n",
      "- Focus on points that appear in at least two explanations, or are clearly consistent.\n",
      "- Provide a clear explanation in 3–6 sentences.\n",
      "\n",
      "--- Prediction & top factors (context) ---\n",
      "Prediction: benign tumor\n",
      "Explanation method: shap\n",
      "Top factors:\n",
      "- worst area = 0.067 (supports)\n",
      "- worst concave points = 0.057 (supports)\n",
      "- mean concave points = 0.042 (supports)\n",
      "- worst radius = 0.040 (supports)\n",
      "- worst perimeter = 0.030 (supports)\n",
      "\n",
      "\n",
      "--- Independent explanations ---\n",
      "\n",
      "Explanation 1:\n",
      "The model predicts a benign tumor based on several key features. The worst area of the tumor is particularly notable, as it strongly supports this prediction. Additionally, the presence of worst concave points and mean concave points also contribute to this conclusion. Furthermore, the worst radius and perimeter of the tumor are also indicative of a benign nature. Overall, these factors collectively suggest that the tumor is likely benign.\n",
      "\n",
      "Explanation 2:\n",
      "The model predicts that this tumor is benign, and the SHAP values suggest that several features contribute to this conclusion. The worst area of the tumor, as well as its concave points, support the prediction of a benign tumor. Additionally, the mean concave points and radius also align with this diagnosis. Furthermore, the worst perimeter of the tumor does not contradict the benign classification, indicating that other factors are more influential in determining the tumor's nature. Overall, these features collectively contribute to the model's confident prediction that this is a benign tumor.\n",
      "\n",
      "Explanation 3:\n",
      "Based on the SHAP explanation, the model predicts a benign tumor because it primarily focuses on the characteristics of the area and shape of the tumor. The worst area and concave points are particularly supportive of this prediction, suggesting that the tumor is not aggressive or invasive. Additionally, the mean concave points and radius also contribute to this conclusion, implying a relatively smooth and rounded tumor shape. Overall, these features collectively support the diagnosis of a benign tumor.\n",
      "\n",
      "--- Task ---\n",
      "Combine the key consistent ideas from these explanations into ONE final explanation.\n",
      "Write only the final explanation, with no preamble or bullet points.\n",
      "\n",
      "================ END OF PROMPT ==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-Consistency (aggregated, technical) ===\n",
      "The model predicts a benign tumor based on several key features that collectively suggest its non-aggressive and non-invasive nature. The worst area of the tumor, along with its concave points, strongly support this prediction, indicating a smooth and rounded shape. Additionally, the mean concave points and radius also contribute to this conclusion, further reinforcing the diagnosis of a benign tumor.\n",
      "\n",
      "================ LLM INPUT PROMPT ================\n",
      "You are an explainable AI assistant (reasoning path 1).\n",
      "You receive model predictions and feature attributions (for example, SHAP or LIME values).\n",
      "Explain this model prediction independently.\n",
      "\n",
      "Rules:\n",
      "- The explanation must be written ONLY in English.\n",
      "- Do NOT invent exact numeric percentages or counts that are not in the input.\n",
      "- Do NOT introduce new features that are not listed in the input.\n",
      "- Respect the 'supports' / 'contradicts' / 'neutral' directions.\n",
      "- Provide a concise explanation in 3–6 sentences.\n",
      "\n",
      "--- Input context ---\n",
      "Prediction: low risk of breast cancer\n",
      "Explanation method: shap\n",
      "Top factors:\n",
      "- worst area = 0.067 (supports)\n",
      "- worst concave points = 0.057 (supports)\n",
      "- mean concave points = 0.042 (supports)\n",
      "- worst radius = 0.040 (supports)\n",
      "- worst perimeter = 0.030 (supports)\n",
      "\n",
      "\n",
      "Now write the explanation:\n",
      "\n",
      "================ END OF PROMPT ==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ LLM INPUT PROMPT ================\n",
      "You are an explainable AI assistant (reasoning path 2).\n",
      "You receive model predictions and feature attributions (for example, SHAP or LIME values).\n",
      "Explain this model prediction independently.\n",
      "\n",
      "Rules:\n",
      "- The explanation must be written ONLY in English.\n",
      "- Do NOT invent exact numeric percentages or counts that are not in the input.\n",
      "- Do NOT introduce new features that are not listed in the input.\n",
      "- Respect the 'supports' / 'contradicts' / 'neutral' directions.\n",
      "- Provide a concise explanation in 3–6 sentences.\n",
      "\n",
      "--- Input context ---\n",
      "Prediction: low risk of breast cancer\n",
      "Explanation method: shap\n",
      "Top factors:\n",
      "- worst area = 0.067 (supports)\n",
      "- worst concave points = 0.057 (supports)\n",
      "- mean concave points = 0.042 (supports)\n",
      "- worst radius = 0.040 (supports)\n",
      "- worst perimeter = 0.030 (supports)\n",
      "\n",
      "\n",
      "Now write the explanation:\n",
      "\n",
      "================ END OF PROMPT ==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ LLM INPUT PROMPT ================\n",
      "You are an explainable AI assistant (reasoning path 3).\n",
      "You receive model predictions and feature attributions (for example, SHAP or LIME values).\n",
      "Explain this model prediction independently.\n",
      "\n",
      "Rules:\n",
      "- The explanation must be written ONLY in English.\n",
      "- Do NOT invent exact numeric percentages or counts that are not in the input.\n",
      "- Do NOT introduce new features that are not listed in the input.\n",
      "- Respect the 'supports' / 'contradicts' / 'neutral' directions.\n",
      "- Provide a concise explanation in 3–6 sentences.\n",
      "\n",
      "--- Input context ---\n",
      "Prediction: low risk of breast cancer\n",
      "Explanation method: shap\n",
      "Top factors:\n",
      "- worst area = 0.067 (supports)\n",
      "- worst concave points = 0.057 (supports)\n",
      "- mean concave points = 0.042 (supports)\n",
      "- worst radius = 0.040 (supports)\n",
      "- worst perimeter = 0.030 (supports)\n",
      "\n",
      "\n",
      "Now write the explanation:\n",
      "\n",
      "================ END OF PROMPT ==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ LLM INPUT PROMPT ================\n",
      "You are an explainable AI assistant.\n",
      "You are given several independent explanations generated for the same model prediction.\n",
      "Your task is to aggregate them into a single, coherent explanation.\n",
      "\n",
      "Rules:\n",
      "- Write the final explanation ONLY in English.\n",
      "- Do NOT invent exact numeric percentages or counts that are not in the input.\n",
      "- Do NOT introduce new features that are not listed in the context or in the explanations.\n",
      "- Focus on points that appear in at least two explanations, or are clearly consistent.\n",
      "- Provide a clear explanation in 3–6 sentences.\n",
      "\n",
      "--- Prediction & top factors (context) ---\n",
      "Prediction: low risk of breast cancer\n",
      "Explanation method: shap\n",
      "Top factors:\n",
      "- worst area = 0.067 (supports)\n",
      "- worst concave points = 0.057 (supports)\n",
      "- mean concave points = 0.042 (supports)\n",
      "- worst radius = 0.040 (supports)\n",
      "- worst perimeter = 0.030 (supports)\n",
      "\n",
      "\n",
      "--- Independent explanations ---\n",
      "\n",
      "Explanation 1:\n",
      "The model predicts a low risk of breast cancer, and this prediction is supported by several key factors. The presence of a worst area, concave points, mean concave points, radius, and perimeter all contribute to this outcome. These features suggest that the tumor's shape and size are not particularly concerning, which aligns with a lower risk of breast cancer.\n",
      "\n",
      "Explanation 2:\n",
      "The model predicts a low risk of breast cancer based on several key factors that contribute to this outcome. The presence of a \"worst area\" and \"worst concave points\" both support this prediction, suggesting that these features are indicative of a benign tumor or a healthy breast tissue. Additionally, the average \"mean concave points\" and \"worst radius\" also support the low risk prediction, implying that the shape and size of the breast tissue are not concerning. Finally, the \"worst perimeter\" factor also supports this outcome, indicating that the overall boundary of the breast tissue is not indicative of cancerous growth.\n",
      "\n",
      "Explanation 3:\n",
      "Based on the SHAP explanation, the model predicts a low risk of breast cancer because it identifies certain characteristics that are favorable for this outcome. Specifically, the model highlights the presence of a \"worst area\" and \"worst concave points\" as supporting factors, indicating that these features do not pose a significant threat to breast health. Additionally, the model notes that the mean concave points, radius, and perimeter all contribute positively to the prediction, suggesting that these aspects are also benign or even beneficial for breast cancer risk assessment. Overall, the combination of these supportive factors leads the model to conclude that the individual has a low risk of developing breast cancer.\n",
      "\n",
      "--- Task ---\n",
      "Combine the key consistent ideas from these explanations into ONE final explanation.\n",
      "Write only the final explanation, with no preamble or bullet points.\n",
      "\n",
      "================ END OF PROMPT ==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Self-Consistency (aggregated, simple) ===\n",
      "The model predicts a low risk of breast cancer due to several supportive factors. The presence of a worst area and worst concave points suggests that the tumor's shape and size are not concerning, aligning with a lower risk of breast cancer. Additionally, the mean concave points, radius, and perimeter all contribute positively to this outcome, indicating that these aspects are benign or even beneficial for breast cancer risk assessment. Overall, the combination of these supportive factors leads the model to conclude that the individual has a low risk of developing breast cancer.\n"
     ]
    }
   ],
   "source": [
    "from src.nlg.self_consistency_generator import SelfConsistencyGenerator\n",
    "\n",
    "# Self-consistency generator: multiple chains + aggregated explanation\n",
    "sc_gen = SelfConsistencyGenerator(\n",
    "    config.nlg,\n",
    "    n_chains=3,               # you can increase this, but latency will go up\n",
    "    llm_call_fn=ollama_llm_call,\n",
    ")\n",
    "\n",
    "# Technical / expert-level aggregated explanation\n",
    "context_sc_expert = {\n",
    "    **base_context,\n",
    "    \"prediction\": pred_expert_en,\n",
    "}\n",
    "\n",
    "# --- Print the prompt used for the first reasoning chain ---\n",
    "# sc_prompt_chain1 = sc_gen.build_chain_prompt(context_sc_expert, chain_id=1)\n",
    "# print(\"=== SELF-CONSISTENCY PROMPT (CHAIN 1, EXPERT) ===\")\n",
    "# print(sc_prompt_chain1)\n",
    "# print(\"=== END SELF-CONSISTENCY PROMPT ===\\n\")\n",
    "\n",
    "text_sc_expert = sc_gen.generate(context_sc_expert)\n",
    "\n",
    "print(\"=== Self-Consistency (aggregated, technical) ===\")\n",
    "print(text_sc_expert)\n",
    "\n",
    "# Simple / layman-style aggregated explanation (using a simpler prediction label)\n",
    "context_sc_layman = {\n",
    "    **base_context,\n",
    "    \"prediction\": pred_layman_en,\n",
    "}\n",
    "\n",
    "text_sc_layman = sc_gen.generate(context_sc_layman)\n",
    "\n",
    "print(\"\\n=== Self-Consistency (aggregated, simple) ===\")\n",
    "print(text_sc_layman)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7e7929",
   "metadata": {},
   "source": [
    "## Full pipeline NLG (Few-Shot vs CoT vs Self-Consistency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c787c6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils:[x] Layer 1: Generating explanations\n",
      "INFO:src.utils:  method: shap\n",
      "INFO:src.utils:[x] Layer 2: Normalizing\n",
      "INFO:src.utils:  features: 30\n",
      "INFO:src.utils:[x] Layer 3: Generating text\n",
      "INFO:src.utils:  generator: few_shot\n",
      "INFO:src.utils:  technique: few_shot\n",
      "INFO:src.utils:  audience: expert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "TECHNIQUE = few_shot\n",
      "==============================\n",
      "\n",
      "================ LLM INPUT PROMPT ================\n",
      "You are an explainable AI assistant.\n",
      "You receive model predictions and feature attributions (for example, SHAP or LIME values).\n",
      "Your task is to generate a faithful natural language explanation based ONLY on the provided factors.\n",
      "\n",
      "You are generating an explanation for a CLINICIAN or DATA SCIENTIST.\n",
      "General rules:\n",
      "- The FINAL EXPLANATION MUST BE WRITTEN ONLY IN ENGLISH.\n",
      "- Do NOT invent exact numeric percentages, thresholds or counts (like '4%', 'three times higher') that are not explicitly present in the input. Use only qualitative terms such as 'larger', 'smaller', 'high', 'low' when describing magnitude, unless the number is given.\n",
      "- Do NOT introduce new features or variables that are not listed in the input.\n",
      "- Stay faithful to the provided factors and their directions (supports / contradicts / neutral).\n",
      "Additional guidelines for expert explanations:\n",
      "- It is acceptable to refer to 'features', 'contributions', and 'model output' using technical language.\n",
      "- Do NOT change the direction provided: 'supports' must be described as supporting the predicted class, and 'contradicts' as opposing or protective with respect to that class.\n",
      "- Do NOT invent mathematical relationships that are not present in the input (for example, new ratios or combined indices).\n",
      "- Explicitly connect the sign and approximate magnitude of contributions with the final conclusion (for example, 'large positive contribution increases the malignancy score').\n",
      "- Highlight which factors are dominant and whether some factors partly counterbalance the decision.\n",
      "- Keep the explanation between 3 and 6 sentences, structured logically from evidence to conclusion.\n",
      "\n",
      "Below are a few EXAMPLES of good explanations (Input and explanation both in English):\n",
      "\n",
      "Example 1:\n",
      "Input:\n",
      "Prediction: malignant tumor\n",
      "Explanation method: SHAP\n",
      "Top factors:\n",
      "- worst_area = 1200.0 (supports)\n",
      "- mean_concave_points = 0.20 (supports)\n",
      "- worst_radius = 18.0 (supports)\n",
      "\n",
      "Explanation (English):\n",
      "The model predicts a malignant tumor because several morphological features have large positive contributions to the malignancy score. A very large worst area and radius are typical of aggressive lesions and strongly support the malignant class, as reflected by their positive SHAP values. In addition, a higher density of concave points further reinforces this pattern. Taken together, these dominant positive contributions outweigh any smaller opposing effects and push the overall prediction clearly towards malignancy.\n",
      "\n",
      "Example 2:\n",
      "Input:\n",
      "Prediction: benign tumor\n",
      "Explanation method: SHAP\n",
      "Top factors:\n",
      "- worst_area = 420.0 (contradicts)\n",
      "- mean_radius = 8.9 (contradicts)\n",
      "- smoothness_mean = 0.08 (supports)\n",
      "\n",
      "Explanation (English):\n",
      "The model predicts a benign tumor because the largest area and average radius of the lesion are within ranges typically seen in non-aggressive nodules and have negative contributions to the malignancy score. These factors contradict a malignant pattern and dominate the overall attribution. Although smoothness_mean has a small positive contribution, its effect is modest compared with the stronger negative contributions. As a result, the combined evidence favours the benign class.\n",
      "\n",
      "--- New instance ---\n",
      "Prediction: 1\n",
      "Explanation method: shap\n",
      "Top factors:\n",
      "- worst area = 0.067 (supports)\n",
      "- worst concave points = 0.057 (supports)\n",
      "- mean concave points = 0.042 (supports)\n",
      "- worst radius = 0.040 (supports)\n",
      "- worst perimeter = 0.030 (supports)\n",
      "\n",
      "\n",
      "Now write the FINAL EXPLANATION in ENGLISH only. Do not invent new features or exact numerical values.\n",
      "\n",
      "================ END OF PROMPT ==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:src.utils:[x] Layer 4: Validating\n",
      "INFO:src.utils:[x] Layer 1: Generating explanations\n",
      "INFO:src.utils:  method: shap\n",
      "INFO:src.utils:[x] Layer 2: Normalizing\n",
      "INFO:src.utils:  features: 30\n",
      "INFO:src.utils:[x] Layer 3: Generating text\n",
      "INFO:src.utils:  generator: cot\n",
      "INFO:src.utils:  technique: cot\n",
      "INFO:src.utils:  audience: expert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reported technique: few_shot\n",
      "Reported generator: few_shot\n",
      "\n",
      "Generated text:\n",
      "\n",
      "The model predicts a positive outcome because several morphological features have small to moderate positive contributions to the overall prediction. The worst area, concave points, mean concave points, radius, and perimeter all provide supporting evidence for this class, with each feature making a relatively similar-sized contribution. These combined positive contributions drive the final prediction towards a positive outcome, with no dominant opposing factors present in the top features.\n",
      "\n",
      "Validation: {'sum_conservation_valid': False, 'computed_sum': 0.97, 'clarity_score': 74.66666666666666}\n",
      "\n",
      "==============================\n",
      "TECHNIQUE = cot\n",
      "==============================\n",
      "\n",
      "================ LLM INPUT PROMPT ================\n",
      "You are an explainable AI assistant.\n",
      "You receive model predictions and feature attributions (for example, SHAP or LIME values).\n",
      "Your task is to reason step by step and then provide a clear explanation of the prediction.\n",
      "\n",
      "Rules:\n",
      "- The final explanation must be written ONLY in English.\n",
      "- Do NOT invent exact numeric percentages or counts that are not in the input.\n",
      "- Do NOT introduce new features that are not listed in the input.\n",
      "- Respect the directions 'supports' / 'contradicts' / 'neutral'.\n",
      "- Use the step-by-step reasoning only as an internal tool; the final explanation should be a clean paragraph.\n",
      "\n",
      "--- Input context ---\n",
      "Prediction: 1\n",
      "Explanation method: shap\n",
      "Top factors:\n",
      "- worst area = 0.067 (supports)\n",
      "- worst concave points = 0.057 (supports)\n",
      "- mean concave points = 0.042 (supports)\n",
      "- worst radius = 0.040 (supports)\n",
      "- worst perimeter = 0.030 (supports)\n",
      "\n",
      "\n",
      "--- Reasoning (you can think step by step here) ---\n",
      "Think step by step about:\n",
      "- What is the predicted outcome?\n",
      "- Which factors have the largest absolute contributions?\n",
      "- Which factors support the prediction and which ones oppose it?\n",
      "- How do these factors jointly justify the final prediction?\n",
      "\n",
      "Do NOT show this internal thought process directly in the final answer.\n",
      "After you finish reasoning, write a concise explanation for the user.\n",
      "\n",
      "--- Final answer ---\n",
      "Now provide ONLY the final explanation in English (3–6 sentences):\n",
      "\n",
      "================ END OF PROMPT ==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:src.utils:[x] Layer 4: Validating\n",
      "INFO:src.utils:[x] Layer 1: Generating explanations\n",
      "INFO:src.utils:  method: shap\n",
      "INFO:src.utils:[x] Layer 2: Normalizing\n",
      "INFO:src.utils:  features: 30\n",
      "INFO:src.utils:[x] Layer 3: Generating text\n",
      "INFO:src.utils:  generator: self_consistency\n",
      "INFO:src.utils:  technique: self_consistency\n",
      "INFO:src.utils:  audience: expert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reported technique: cot\n",
      "Reported generator: cot\n",
      "\n",
      "Generated text:\n",
      "\n",
      "The predicted outcome is that the object will be classified as \"1\". The top factors contributing to this prediction are all related to the shape and size of the object's boundary, with the worst area, concave points, radius, and perimeter all playing a supportive role. These features suggest that the object has a complex and irregular shape, which is consistent with the predicted classification. Overall, the combination of these factors leads to a strong indication that the object belongs to class \"1\".\n",
      "\n",
      "Validation: {'sum_conservation_valid': False, 'computed_sum': 0.97, 'clarity_score': 79.0}\n",
      "\n",
      "==============================\n",
      "TECHNIQUE = self_consistency\n",
      "==============================\n",
      "\n",
      "================ LLM INPUT PROMPT ================\n",
      "You are an explainable AI assistant (reasoning path 1).\n",
      "You receive model predictions and feature attributions (for example, SHAP or LIME values).\n",
      "Explain this model prediction independently.\n",
      "\n",
      "Rules:\n",
      "- The explanation must be written ONLY in English.\n",
      "- Do NOT invent exact numeric percentages or counts that are not in the input.\n",
      "- Do NOT introduce new features that are not listed in the input.\n",
      "- Respect the 'supports' / 'contradicts' / 'neutral' directions.\n",
      "- Provide a concise explanation in 3–6 sentences.\n",
      "\n",
      "--- Input context ---\n",
      "Prediction: 1\n",
      "Explanation method: shap\n",
      "Top factors:\n",
      "- worst area = 0.067 (supports)\n",
      "- worst concave points = 0.057 (supports)\n",
      "- mean concave points = 0.042 (supports)\n",
      "- worst radius = 0.040 (supports)\n",
      "- worst perimeter = 0.030 (supports)\n",
      "\n",
      "\n",
      "Now write the explanation:\n",
      "\n",
      "================ END OF PROMPT ==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ LLM INPUT PROMPT ================\n",
      "You are an explainable AI assistant (reasoning path 2).\n",
      "You receive model predictions and feature attributions (for example, SHAP or LIME values).\n",
      "Explain this model prediction independently.\n",
      "\n",
      "Rules:\n",
      "- The explanation must be written ONLY in English.\n",
      "- Do NOT invent exact numeric percentages or counts that are not in the input.\n",
      "- Do NOT introduce new features that are not listed in the input.\n",
      "- Respect the 'supports' / 'contradicts' / 'neutral' directions.\n",
      "- Provide a concise explanation in 3–6 sentences.\n",
      "\n",
      "--- Input context ---\n",
      "Prediction: 1\n",
      "Explanation method: shap\n",
      "Top factors:\n",
      "- worst area = 0.067 (supports)\n",
      "- worst concave points = 0.057 (supports)\n",
      "- mean concave points = 0.042 (supports)\n",
      "- worst radius = 0.040 (supports)\n",
      "- worst perimeter = 0.030 (supports)\n",
      "\n",
      "\n",
      "Now write the explanation:\n",
      "\n",
      "================ END OF PROMPT ==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ LLM INPUT PROMPT ================\n",
      "You are an explainable AI assistant (reasoning path 3).\n",
      "You receive model predictions and feature attributions (for example, SHAP or LIME values).\n",
      "Explain this model prediction independently.\n",
      "\n",
      "Rules:\n",
      "- The explanation must be written ONLY in English.\n",
      "- Do NOT invent exact numeric percentages or counts that are not in the input.\n",
      "- Do NOT introduce new features that are not listed in the input.\n",
      "- Respect the 'supports' / 'contradicts' / 'neutral' directions.\n",
      "- Provide a concise explanation in 3–6 sentences.\n",
      "\n",
      "--- Input context ---\n",
      "Prediction: 1\n",
      "Explanation method: shap\n",
      "Top factors:\n",
      "- worst area = 0.067 (supports)\n",
      "- worst concave points = 0.057 (supports)\n",
      "- mean concave points = 0.042 (supports)\n",
      "- worst radius = 0.040 (supports)\n",
      "- worst perimeter = 0.030 (supports)\n",
      "\n",
      "\n",
      "Now write the explanation:\n",
      "\n",
      "================ END OF PROMPT ==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ LLM INPUT PROMPT ================\n",
      "You are an explainable AI assistant.\n",
      "You are given several independent explanations generated for the same model prediction.\n",
      "Your task is to aggregate them into a single, coherent explanation.\n",
      "\n",
      "Rules:\n",
      "- Write the final explanation ONLY in English.\n",
      "- Do NOT invent exact numeric percentages or counts that are not in the input.\n",
      "- Do NOT introduce new features that are not listed in the context or in the explanations.\n",
      "- Focus on points that appear in at least two explanations, or are clearly consistent.\n",
      "- Provide a clear explanation in 3–6 sentences.\n",
      "\n",
      "--- Prediction & top factors (context) ---\n",
      "Prediction: 1\n",
      "Explanation method: shap\n",
      "Top factors:\n",
      "- worst area = 0.067 (supports)\n",
      "- worst concave points = 0.057 (supports)\n",
      "- mean concave points = 0.042 (supports)\n",
      "- worst radius = 0.040 (supports)\n",
      "- worst perimeter = 0.030 (supports)\n",
      "\n",
      "\n",
      "--- Independent explanations ---\n",
      "\n",
      "Explanation 1:\n",
      "The model predicts that this shape is a circle, and it's likely because of several features that suggest it has a high degree of symmetry. The presence of worst area, worst concave points, mean concave points, worst radius, and worst perimeter all support this conclusion. These features indicate that the shape lacks any significant irregularities or asymmetries, which is consistent with the characteristics of a circle.\n",
      "\n",
      "Explanation 2:\n",
      "The model predicted that this shape is a circle, with a confidence of 1. The top factors supporting this prediction are the presence of worst area, worst concave points, mean concave points, worst radius, and worst perimeter. These features all contribute to the overall circularity of the shape, indicating that it has no sharp corners or irregularities.\n",
      "\n",
      "Explanation 3:\n",
      "The model predicted that this shape is a circle, with a confidence level of 1. The top factors contributing to this prediction are all related to the shape's geometry. Specifically, the presence of worst area, worst concave points, mean concave points, worst radius, and worst perimeter all support the conclusion that this shape is a circle. These features suggest that the shape has a consistent curvature and no sharp edges or irregularities, which are characteristic of circular shapes.\n",
      "\n",
      "--- Task ---\n",
      "Combine the key consistent ideas from these explanations into ONE final explanation.\n",
      "Write only the final explanation, with no preamble or bullet points.\n",
      "\n",
      "================ END OF PROMPT ==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:src.utils:[x] Layer 4: Validating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reported technique: self_consistency\n",
      "Reported generator: self_consistency\n",
      "\n",
      "Generated text:\n",
      "\n",
      "The model predicts that this shape is a circle due to several features that suggest high symmetry and lack of significant irregularities. The presence of worst area, worst concave points, mean concave points, worst radius, and worst perimeter all contribute to the overall circularity of the shape, indicating consistent curvature and no sharp edges or corners.\n",
      "\n",
      "Validation: {'sum_conservation_valid': False, 'computed_sum': 0.97, 'clarity_score': 64.0}\n"
     ]
    }
   ],
   "source": [
    "# End-to-end test: let the pipeline handle XAI + NLG\n",
    "# We only switch the NLG technique: few_shot / cot / self_consistency\n",
    "\n",
    "for tech in [\"few_shot\", \"cot\", \"self_consistency\"]:\n",
    "    print(\"\\n==============================\")\n",
    "    print(f\"TECHNIQUE = {tech}\")\n",
    "    print(\"==============================\")\n",
    "\n",
    "    res = pipeline.explain_instance(\n",
    "        instance,\n",
    "        method=\"shap\",\n",
    "        generate_text=True,\n",
    "        technique=tech,      # This selects the NLG generator inside the pipeline\n",
    "        audience=\"expert\",   # Audience is only used by FewShotGenerator\n",
    "    )\n",
    "\n",
    "    print(\"Reported technique:\", res.get(\"nlg_technique\"))\n",
    "    print(\"Reported generator:\", res.get(\"nlg_generator\"))\n",
    "    print(\"\\nGenerated text:\\n\")\n",
    "    print(res.get(\"generated_text\", \"\")[:800])  # print only the first 800 characters\n",
    "    print(\"\\nValidation:\", res.get(\"validation\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
