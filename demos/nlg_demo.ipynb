{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f8fd143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shap in c:\\users\\mereu\\miniconda3\\lib\\site-packages (0.49.1)\n",
      "Requirement already satisfied: lime in c:\\users\\mereu\\miniconda3\\lib\\site-packages (0.2.0.1)\n",
      "Requirement already satisfied: ollama in c:\\users\\mereu\\miniconda3\\lib\\site-packages (0.6.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\mereu\\miniconda3\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from shap) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from shap) (1.16.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from shap) (1.7.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from shap) (2.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from shap) (25.0)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from shap) (0.62.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from shap) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from shap) (4.15.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from lime) (3.9.2)\n",
      "Requirement already satisfied: scikit-image>=0.12 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from lime) (0.25.2)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from ollama) (0.27.2)\n",
      "Requirement already satisfied: pydantic>=2.9 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from ollama) (2.10.5)\n",
      "Requirement already satisfied: anyio in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from httpx>=0.27->ollama) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from httpx>=0.27->ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from httpx>=0.27->ollama) (1.0.6)\n",
      "Requirement already satisfied: idna in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from httpx>=0.27->ollama) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from httpx>=0.27->ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.14.0)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from numba>=0.54->shap) (0.45.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from pydantic>=2.9->ollama) (2.27.2)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (3.2.1)\n",
      "Requirement already satisfied: pillow>=10.1 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (10.4.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (2025.6.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from scikit-learn->shap) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from scikit-learn->shap) (3.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from matplotlib->lime) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from matplotlib->lime) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from matplotlib->lime) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from matplotlib->lime) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from matplotlib->lime) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from matplotlib->lime) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mereu\\miniconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->lime) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install shap lime ollama python-dotenv\n",
    "#!pip install \"numpy==1.26.4\" --force-reinstall\n",
    "#!pip install \"shap<0.50\" lime ollama python-dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae9ba4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# DacÄƒ rulezi notebook-ul din folderul demo/, cwd = .../xai-nlg-framework/demo\n",
    "# Deci root-ul proiectului e un nivel mai sus:\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if ROOT_DIR not in sys.path:\n",
    "    sys.path.append(ROOT_DIR)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import src.pipeline\n",
    "importlib.reload(src.pipeline)\n",
    "\n",
    "from config.settings import FrameworkConfig\n",
    "from src.pipeline import XAINLGPipeline\n",
    "from src.nlg.few_shot_generator import FewShotGenerator\n",
    "from src.nlg.ollama_client import ollama_llm_call\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6321235e",
   "metadata": {},
   "source": [
    "dataset + model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ce06dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455, 114, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = list(data.feature_names)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 3. Config + pipeline\n",
    "config = FrameworkConfig(verbose = True)\n",
    "config.explainer.top_k_features = 5 \n",
    "pipeline = XAINLGPipeline(\n",
    "    model,\n",
    "    X_train,\n",
    "    feature_names,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "\n",
    "len(X_train), len(X_test), len(feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33979e7",
   "metadata": {},
   "source": [
    "pipeline + explicaÈ›ie SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e8b7bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils:[x] Layer 1: Generating explanations\n",
      "INFO:src.utils:  method: shap\n",
      "INFO:src.utils:[x] Layer 2: Normalizing\n",
      "INFO:src.utils:  features: 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw prediction: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['instance', 'method', 'explanation', 'base_value', 'prediction', 'ranked_features', 'statements'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alegem o instanÈ›Äƒ de explicat\n",
    "instance = X_test[0]\n",
    "raw_pred = int(model.predict(instance.reshape(1, -1))[0])\n",
    "print(\"Raw prediction:\", raw_pred)\n",
    "\n",
    "# generÄƒm explicaÈ›ia SHAP (fÄƒrÄƒ NLG intern)\n",
    "result = pipeline.explain_instance(\n",
    "    instance,\n",
    "    method=\"shap\",\n",
    "    generate_text=False,   # NLG Ã®l facem noi Ã®n notebook\n",
    ")\n",
    "\n",
    "result.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ac5777",
   "metadata": {},
   "source": [
    "NLG cu Few-Shot + Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c8a4622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worst area', 0.06724261426271144),\n",
       " ('worst concave points', 0.05740842121890832),\n",
       " ('mean concave points', 0.04227830540602773),\n",
       " ('worst radius', 0.03956915260512325),\n",
       " ('worst perimeter', 0.03026387093604719)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Layer 3: NLG cu Ollama (Few-Shot) ===\n",
    "\n",
    "# generator NLG care foloseÈ™te llama3:latest din Ollama\n",
    "nlg = FewShotGenerator(config.nlg, llm_call_fn=ollama_llm_call)\n",
    "\n",
    "# Top-k features din explicaÈ›ia SHAP\n",
    "top_k = result[\"ranked_features\"][: config.explainer.top_k_features]\n",
    "top_k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3884384",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directions = []\n",
    "for _, v in top_k:\n",
    "    if v > 0:\n",
    "        directions.append(\"supports\")\n",
    "    elif v < 0:\n",
    "        directions.append(\"contradicts\")\n",
    "    else:\n",
    "        directions.append(\"neutral\")\n",
    "\n",
    "raw_pred = int(result[\"prediction\"])\n",
    "\n",
    "pred_expert_en = \"malignant tumor\" if raw_pred == 0 else \"benign tumor\"\n",
    "pred_layman_en = \"high risk of breast cancer\" if raw_pred == 0 else \"low risk of breast cancer\"\n",
    "\n",
    "base_context = {\n",
    "    \"features\": [f for f, _ in top_k],\n",
    "    \"values\": [float(v) for _, v in top_k],\n",
    "    \"directions\": directions,\n",
    "    \"method\": result.get(\"method\", \"shap\"),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7e7929",
   "metadata": {},
   "source": [
    "## generÄƒm efectiv textul cu Llama3 prin Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c787c6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExplicaÈ›ie tehnicÄƒ (pentru medic sau cercetÄƒtor):\n",
      "The model predicts a benign tumor because several morphological features have small positive contributions to the malignancy score. The worst area, concave points, radius, and perimeter all exhibit low values that support the benign class, with their combined effects dominating the overall attribution. These small but consistent positive contributions outweigh any potential opposing factors, ultimately leading to a prediction of a benign tumor.\n",
      "\n",
      "Pe scurt, pentru un pacient:\n",
      "The model suggests a low risk of breast cancer because the lump's size and shape are quite small and regular, which is more typical of harmless findings. The texture measurements also point towards a benign outcome, indicating that this lump does not have the characteristics commonly seen in cancer cases. Overall, these factors together suggest that the lump is likely to be non-cancerous.\n"
     ]
    }
   ],
   "source": [
    "from src.nlg.few_shot_generator import FewShotGenerator\n",
    "from src.nlg.ollama_client import ollama_llm_call\n",
    "\n",
    "# puÈ›in mai â€žcuminteâ€ pentru format fix\n",
    "config.nlg.temperature = 0.2  \n",
    "\n",
    "nlg = FewShotGenerator(config.nlg, llm_call_fn=ollama_llm_call)\n",
    "\n",
    "# 1) context pentru EXPERT\n",
    "context_expert = {\n",
    "    **base_context,\n",
    "    \"prediction\": pred_expert_en,   # ex: \"malignant tumor\" / \"benign tumor\"\n",
    "    \"audience\": \"expert\",\n",
    "}\n",
    "\n",
    "# 2) context pentru LAYMAN\n",
    "context_layman = {\n",
    "    **base_context,\n",
    "    \"prediction\": pred_layman_en,   # ex: \"high risk of breast cancer\" / \"low risk of breast cancer\"\n",
    "    \"audience\": \"layman\",\n",
    "}\n",
    "\n",
    "text_expert = nlg.generate(context_expert)\n",
    "text_layman = nlg.generate(context_layman)\n",
    "\n",
    "print(\"ExplicaÈ›ie tehnicÄƒ (pentru medic sau cercetÄƒtor):\")\n",
    "print(text_expert)\n",
    "print(\"\\nPe scurt, pentru un pacient:\")\n",
    "print(text_layman)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c6549a",
   "metadata": {},
   "source": [
    "## Chain of Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493d7404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_top_k_features\n",
    "\n",
    "# ranked_features = list of (feature_name, value)\n",
    "top_k = result[\"ranked_features\"][: config.explainer.top_k_features]\n",
    "\n",
    "directions = []\n",
    "for _, v in top_k:\n",
    "    if v > 0:\n",
    "        directions.append(\"supports\")\n",
    "    elif v < 0:\n",
    "        directions.append(\"contradicts\")\n",
    "    else:\n",
    "        directions.append(\"neutral\")\n",
    "\n",
    "raw_pred = int(result[\"prediction\"])\n",
    "\n",
    "pred_expert_en = \"malignant tumor\" if raw_pred == 0 else \"benign tumor\"\n",
    "pred_layman_en = \"high risk of breast cancer\" if raw_pred == 0 else \"low risk of breast cancer\"\n",
    "\n",
    "base_context = {\n",
    "    \"features\": [f for f, _ in top_k],\n",
    "    \"values\": [float(v) for _, v in top_k],\n",
    "    \"directions\": directions,\n",
    "    \"method\": result.get(\"method\", \"shap\"),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8948765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nlg.cot_generator import ChainOfThoughtGenerator\n",
    "from src.nlg.ollama_client import ollama_llm_call\n",
    "\n",
    "cot_gen = ChainOfThoughtGenerator(config.nlg, llm_call_fn=ollama_llm_call)\n",
    "\n",
    "context_cot_expert = {\n",
    "    **base_context,\n",
    "    \"prediction\": pred_expert_en,\n",
    "}\n",
    "\n",
    "text_cot_expert = cot_gen.generate(context_cot_expert)\n",
    "\n",
    "print(\"=== Chain-of-Thought (technical) ===\")\n",
    "print(text_cot_expert)\n",
    "\n",
    "context_cot_layman = {\n",
    "    **base_context,\n",
    "    \"prediction\": pred_layman_en,\n",
    "}\n",
    "\n",
    "text_cot_layman = cot_gen.generate(context_cot_layman)\n",
    "\n",
    "print(\"\\n=== Chain-of-Thought (simple) ===\")\n",
    "print(text_cot_layman)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef6fe91",
   "metadata": {},
   "source": [
    "## Self_conistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bbac572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-Consistency (aggregated, technical) ===\n",
      "The model predicts a benign tumor based on several key factors. The worst area and concave points of the tumor strongly support this prediction, indicating that its shape is not irregular or suspicious. Additionally, the mean concave points and radius contribute to the conclusion that it is benign, suggesting that the tumor's overall structure and dimensions are consistent with a benign diagnosis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Self-Consistency (aggregated, simple) ===\n",
      "The model's prediction of a low risk of breast cancer is supported by several favorable features in the input data. The presence of worst area, worst concave points, mean concave points, worst radius, and worst perimeter all contribute to this outcome, indicating that the tumor has a relatively benign shape and size. Specifically, the less concerning worst area and concave points suggest a less aggressive growth pattern, while the mean concave points and radius indicate a more symmetrical and compact growth pattern, further supporting a low risk of breast cancer diagnosis.\n"
     ]
    }
   ],
   "source": [
    "from src.nlg.self_consistency_generator import SelfConsistencyGenerator\n",
    "\n",
    "sc_gen = SelfConsistencyGenerator(\n",
    "    config.nlg,\n",
    "    n_chains=3,                    # poÈ›i pune 3â€“5, dar È›ine cont de latenÈ›Äƒ\n",
    "    llm_call_fn=ollama_llm_call,\n",
    ")\n",
    "\n",
    "context_sc_expert = {\n",
    "    **base_context,\n",
    "    \"prediction\": pred_expert_en,\n",
    "}\n",
    "\n",
    "text_sc_expert = sc_gen.generate(context_sc_expert)\n",
    "\n",
    "print(\"=== Self-Consistency (aggregated, technical) ===\")\n",
    "print(text_sc_expert)\n",
    "\n",
    "context_sc_layman = {\n",
    "    **base_context,\n",
    "    \"prediction\": pred_layman_en,\n",
    "}\n",
    "\n",
    "text_sc_layman = sc_gen.generate(context_sc_layman)\n",
    "\n",
    "print(\"\\n=== Self-Consistency (aggregated, simple) ===\")\n",
    "print(text_sc_layman)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ce6a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tech in [\"few_shot\", \"cot\", \"self_consistency\"]:\n",
    "    print(\"\\n==============================\")\n",
    "    print(f\"TECHNIQUE = {tech}\")\n",
    "    print(\"==============================\")\n",
    "\n",
    "    res = pipeline.explain_instance(\n",
    "        instance,\n",
    "        method=\"shap\",\n",
    "        generate_text=True,\n",
    "        technique=tech,        # ðŸ‘ˆ AICI testÄƒm ramura\n",
    "        audience=\"expert\",     # pentru few_shot; la celelalte e ignorat\n",
    "    )\n",
    "\n",
    "    print(\"Reported technique:\", res.get(\"nlg_technique\"))\n",
    "    print(\"Reported generator:\", res.get(\"nlg_generator\"))\n",
    "    print(\"\\nGenerated text:\\n\")\n",
    "    print(res.get(\"generated_text\", \"\")[:800])  # primele 800 caractere\n",
    "    print(\"\\nValidation:\", res.get(\"validation\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
